# Python Integration Guide

This document explains how the Python script integration works in this Docker-based Node.js/Express application.

## Overview

The application now runs a Python script (`process_data.py`) **before** starting the Node.js server. The Python script generates JSON files in the `input_file` folder, which are then loaded into PostgreSQL by the Node.js application.

## How It Works

### 1. **Entrypoint Script** (`entrypoint.sh`)
- Runs when the container starts
- Executes the Python script first
- Then starts the Node.js application

### 2. **Python Script** (`process_data.py`)
- Place your Python data processing logic here
- Output files should be saved to `/app/input_file/` directory
- Currently includes a sample implementation

### 3. **Dockerfile Updates**
- Installs Python 3 and pip in the Alpine container
- Sets up the entrypoint script
- Ensures proper permissions

### 4. **Docker Compose**
- `input_file` volume is mounted as read-write (removed `:ro`)
- Allows Python script to write files to the folder
- No changes needed to existing services

## Usage

### 1. Update the Python Script

Edit `process_data.py` with your actual data processing logic:

```python
def main():
    # Your Python logic here
    # Save output to /app/input_file/
    pass
```

### 2. Rebuild and Run

```bash
# Rebuild the Docker image
docker-compose build

# Start the services
docker-compose up -d

# Or with Podman
podman-compose build
podman-compose up -d
```

### 3. Check Logs

```bash
# Check Python execution
docker logs data_ingest_app | grep "üêç"

# Check full logs
docker logs -f data_ingest_app
```

## File Structure

```
my-data-ingest-app/
‚îú‚îÄ‚îÄ process_data.py          # Your Python script (edit this!)
‚îú‚îÄ‚îÄ entrypoint.sh            # Startup script (runs Python then Node.js)
‚îú‚îÄ‚îÄ Dockerfile               # Updated to include Python
‚îú‚îÄ‚îÄ docker-compose.yml       # No changes needed
‚îú‚îÄ‚îÄ input_file/              # Output folder for Python-generated files
‚îÇ   ‚îú‚îÄ‚îÄ consolidated_charge.json
‚îÇ   ‚îú‚îÄ‚îÄ consolidated_rate.json
‚îÇ   ‚îî‚îÄ‚îÄ ... (generated by Python)
‚îî‚îÄ‚îÄ server.js                # Node.js app (loads from input_file/)
```

## Execution Flow

1. **Container Starts** ‚Üí Entrypoint script runs
2. **Python Script Executes** ‚Üí Generates JSON files in `input_file/`
3. **Node.js App Starts** ‚Üí Loads JSON files into PostgreSQL
4. **API Endpoints Ready** ‚Üí Serve data via REST API

## Important Notes

- The Python script runs **every time** the container starts
- Make sure your Python script is idempotent (safe to run multiple times)
- If the Python script fails, the Node.js app won't start
- Files in `input_file/` are **not read-only** anymore (for Python to write)

## Troubleshooting

### Python script not found
- Ensure `process_data.py` is in the project root
- Check file is not in `.dockerignore`

### Permission errors
- The entrypoint runs as root, then switches to nodejs user for Node app
- Python script runs with necessary permissions

### Files not appearing in input_file
- Check container logs for errors
- Verify `input_file` volume mount in docker-compose.yml
- Ensure no `:ro` (read-only) flag on the volume

## Example: Adding Dependencies

If your Python script needs additional packages, update the Dockerfile:

```dockerfile
RUN pip3 install pandas numpy  # Add your packages
```

## Alternative Approach: Separate Python Service

If you prefer Python running in a separate container:

1. Add a `python` service in `docker-compose.yml`
2. Set `app` to depend on `python` service
3. Share volumes between services

This approach is useful if:
- Python processing is long-running
- You want separate logs
- Python needs different dependencies

